<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="./css/style.css">
    <title>CSE163 Archive</title>
  </head>
  <body>
    
    
    

    <div class="container-fluid px-0 m-0 navbar navbar-dark navbar-expand-lg nav-color" id="mainNav">
      <!-- Content here -->
      <nav class="container m-0">
	<!-- Navbar content -->
	
	<a class="navbar-brand mr-5 font-weight-bold text-warning" href="#">CSE163</a>
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
	  <span class="navbar-toggler-icon"></span>
	</button>

	<div class="collapse navbar-collapse" id="navbarSupportedContent">
	  <ul class="navbar-nav mr-auto">
	    <li class="nav-item active">
	      <a class="nav-link" href="./index.html#">Home <span class="sr-only">(current)</span></a>
	    </li>
	    <li class="nav-item dropdown">
	      <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
		Assignments
	      </a>

	    </li>
	  </ul>
	</div>
	
      </nav>
      <!-- end of container -->
    </div>

    <div class="jumbotron">

      <div class="container">
	
	<h1 class="display-4">Light Field Viewer</h1>
	<hr class="my-4">
      </div>

    </div>
    

    <div class="essay container">
      
      <dl class="row">
	<h1 class="display-5 col-md-8">INTRODUCTION</h1>
      </dl>
      


      
      <p>One of the exciting up-and-coming fields of research in computer graphics is image-based rendering. This type of rendering differs from the traditional methods in that it uses images of the scene rather than the 3D model of the scene.</p>
      <p>In its basic form, it simply interpolates between the images to create a continuous view of the scene. This is done by defining two planes – the UV plane and the ST plane. The UV plane represents the camera (each point corresponding to the camera’s position), while the ST planes represents the image of the scene (each point corresponding to a pixel in the image). Next, each point in the UV plane maps to an array of points in the ST plane. What this means is that each camera position in the UV plane maps to an image (2D array of pixels) in the ST plane. A viewer then places a camera in space, not necessarily on the UV plane, and uses ray tracing to find which pixel out of all the possible images and camera positions to render to the screen. </p>


      <hr class="my-4 my-4">
      <dl class="row mt-5">
	<h1 class="display-5 col-md-8">PROPOSAL</h1>
      </dl>
      <p>The goal of our project is to create a basic light field viewer that uses the image-based rendering technique. Our baseline implementation would allow the user to translate the camera in the xyz-axes. It would also have the three different interpolation methods: nearest neighbor, bilinear, and quadrilinear. If time allows, we would add the following functionalities:</p>
      <ol>
	<li>
	  <p>Allow the user to change the viewing angle of the camera </p>
	</li>
	      
	<li>	
	  <p>Have an aperture and focal plane </p>
	</li>
	      
      </ol>



      <dl class="row mt-5">
	<h1 class="display-5 col-md-8">PROGRESS REPORT (June 1, 2018)</h1>
      </dl>

      <p>To start our project, we have created a synthetic dataset using the <a href="http://www.pbrt.org/">PBRT program created by Pharr, Jakob, and Humphreys</a>. Here is the link to their github repository. <INCLUDE LINK> We created a script to run the program to render 17x17 images with resolutions of 512x512 pixels. Each image corresponds to the scene being captured by a camera positioned at (x, y) where the range of both x and y are [-40, 40], with a gap of 5 in between each camera.
      </p>

      <div class="row pb-5 align-items-center">
	      <div class="col-md-5 mx-auto">
		<img src="./images/hw3/glassBall.png" class="img-thumbnail">
		<p class="text-center mx-auto"><small>Scene rendered with camera positioned at (0, 0)</small></p>
	      </div>
      </div>

      <p>As a next step, we have started on the light field viewer. We have implemented the program to take in a directory containing the images, and it loads them into the ImagePlane class. This ImagePlane class parses each image’s name, which contains the x and y positions of the camera, and places them in an array of images. It then uses the current camera’s position and the nearest neighbor algorithm to decide which pixels are rendered onto the screen.
      </p>

      <div class="row pb-5 align-items-center">
	<div class="col-md-5 mx-auto">
	  <img src="./images/hw3/lightfield.png" class="img-thumbnail">
	  <p class="text-center mx-auto"><small>Downsized (9x9 instead of 17x17) visualization of the UV plane</small></p>
	</div>
      </div>

      <p>However, as of the writing of this report, our viewer still has bugs regarding the mapping of the pixels in the input images to the pixels rendered on the screen. Below are sample screenshots of this issue.
      </p>
      <div class="row pb-5 align-items-center">
	<div class="col-md-5 mx-auto">
	  <img src="./images/hw3/bug.png" class="img-thumbnail">
	  <p class="text-center mx-auto"><small>Buggy Program</small></p>
	</div>
      </div>
	    
     <hr class="my-4 my-4">
	    
     <dl class="row mt-5">
	<h1 class="display-5 col-md-8">Class Design</h1>
      </dl>    
	    
	    <h4> VirtualCamera </h4>
	    <p> This class can be thought of as the center class. It contains information about the viewer's camera, the camera plane, 
		and the focal plane. Because of this, the functions to interact with the camera through translations and rotations, the 
		functions for ray tracing and interpolation, as well as the functions for rendering the pixels into a buffer are found 
		in this class. </p>
	    
	    <h4> ImagePlane </h4>
	    <p> Acting as the camera plane's class, this class mainly contains the array of Images that represent the image plane and 
		whose camera positions represent the camera plane. This class is responsible for the logic that loads the images from a 
		directory into memory as well as placing the images in their appropriate indices in the image array. </p>
	    
	    <h4> Image </h4>
	    <p> As the name suggests, this class represents a single image in the image plane. It holds an array of pixels, the 
		dimensions of the image, and the position of the camera on the camera plane where the image was rendered. It also holds 
		the function to read in an image file into memory. </p>
	    
	    
     <dl class="row mt-5">
	<h1 class="display-5 col-md-8">Light Field Viewer</h1>
      </dl>  
	    <h4> Display </h4>
	    <p> TODO </p>
	    
	    <h4> Camera Interaction </h4>
	    <p> TODO </p>
	    
     <dl class="row mt-5">
	<h1 class="display-5 col-md-8">Data Set</h1>
      </dl>  
	    
	    <h4> Synthetic Dataset </h4>
	    <p> As part of the first milestone, we had generated a synthetic dataset to be used for the viewer. This is done by running 
		a script to configure the PBRT with different camera positions and execute it to render the scene. As a result of this, 
		the synthetic dataset captured different parts of the focal plane. The ray tracing method used in light field rendering, 
		however, requires that each of the images in the dataset show the same part of the focal plane but in different camera 
		positions. This requires modifying the view frustrum depending on the camera position, and, given the possible inputs 
		to PBRT, it has proven to be difficult to accomplish. To work around this, we instead decided to fix the camera's 
		look-at point. While the resulting images do not exactly match the requirements, this method provides a rough estimate 
		that is good enough to be used in the viewer. </p>
	    
	    <h4> Stanford Dataset </h4>
	    <p> As a proof of concept, we also utilized two datasets provided in Stanford Computer Graphics Laboratory's old light field 
		data archives: the dragon and buddha datasets. Both datasets have 32x32 images, each with a dimension of 256x256 pixels.
	    </p>
	    
	    <div class="row pt-5">
	    	<div class="col-md-6 text-center m-auto">
			<img src="./images/hw3/gifs/outward_rotation.gif " class="img-fluid" alt="original_wireframe">
			<p class = "text-center m-auto"><small> Buddha </small> </p>
		</div>
	    </div>
	    
	    
      <dl class="row mt-5">
	<h1 class="display-5 col-md-8">Ray Tracing</h1>
      </dl>  
	    
	    <h4> Camera Plane </h4>
	    <p> TODO </p>
	    
	    <h4> Focal Plane </h4>
	    <p> TODO </p>
	    
	    <h4> Interpolation Method </h4>
	    <ol>
	      <li>
		<h3> Nearest Neighbor </h3>
		<p> It's very easy to implement nearest neighbor sampling in the lightfield viewer using ray tracing. From a camera position, we shoot out very single ray to each pixel on the view port. Then, tracing the ray, we find the intersected points on UV plane(Camera Plane) and ST plane(Focal Plane). Then, from the real
		  value instersecting points, we find the nearest grid point on the two planes to retrieve the corrosponding pixel color from them and color it on the viewport to
		  render our "new" images.</p>
		<div class="row pt-5">
	    	  <div class="col-md-6">
		    <img src="./images/hw3/gifs/nearest_neightbor.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small> The glass ball </small> </p>
		  </div>
		  <div class="col-md-6">
		    <img src="./images/hw3/gifs/dragon_nearest.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small> Dragon </small> </p>
		  </div>

		</div>

		    </li>
		    
		    <li> <h3> Bilinear (ST) </h3>
		      <p>To further remove the alising on the image, we use a bilinear interpolation on the image plane(ST). As a result, when we zoom in, we will find out
		      the aliasing effect caused by scaling up would be smoothed out a little.</p>
		    </li>
		    <div class="row pt-5">
	    	      <div class="text-center col-md-6 m-auto">
			<img src="./images/hw3/gifs/with_or_without_bilinear_st.gif" class="img-fluid" alt="original_wireframe">
			<p class = "text-center m-auto"><small> With or without bilinear on ST </small> </p>
		      </div>
		    </div>
		    
		    <li> <h3> Bilinear (UV) </h3>
		      <p>Similarly, in order to remove the aliasing caused by movement of the virtual camera around the sence, we apply bilinear interpolation on UV plane
			to average out one point's pixel from two different camera. As a result, we can see a bluring effect when a ray lands between two different camera position
			instead of dramatic zigzag aliasing.</p>
		      <div class="row pt-5">
	    		<div class="col-md-6">
			  <img src="./images/hw3/gifs/bilinear_on_uv.gif" class="img-fluid" alt="original_wireframe">
			  <p class = "text-center m-auto"><small>Fly around using bilinear on UV </small> </p>
			</div>
			<div class="col-md-6">
			  <img src="./images/hw3/gifs/bi_near.gif" class="img-fluid" alt="original_wireframe">
			  <p class = "text-center m-auto"><small>Difference between bilinear and nearest neightbor </small> </p>
			</div>
		      </div>		      
		    </li>
		    
	      <li>
		<h3> Quadrilinear </h3>
		<p> Combining both bilinear interpolation on UV and ST planes, we can achieve a satisfactory results. For one ray and one pixel on the viewport, we consider
		  16 different pixel values.
		</p>

		<div class="row">
		  <div class="col-md-4">
		    <img src="./images/hw3/gifs/qualinear.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small>Fly around on Glass Ball </small> </p>
		  </div>
		  <div class="col-md-4">
		    <img src="./images/hw3/gifs/bi_qd.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small>Bilinear on UV to Quadrilinear transition on Glass Ball</small> </p>
		  </div>
		  <div class="col-md-4">
		    <img src="./images/hw3/gifs/bilinear_qualinear.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small>Bilinear on UV to Quadrilinear transition on buddha</small> </p>
		  </div>

		</div>

	      </li>
	      <li>
		<h3> Three different views using different interpolation method (Nearest, Bilinear on UV and Quadrilinear on ST) </h3>
		<p>Check the animation below </p>
		<div class="row">
		  <div class="col-md-4">
		    <img src="./images/hw3/gifs/dragon_nearest.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small>Dragon Nearest </small> </p>
		  </div>

		  <div class="col-md-4">
		    <img src="./images/hw3/gifs/dragon_bilinear_uv.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small>Dragon Bilinear </small> </p>
		  </div>
		  
		  <div class="col-md-4">
		    <img src="./images/hw3/gifs/dragon_qualinear.gif" class="img-fluid" alt="original_wireframe">
		    <p class = "text-center m-auto"><small>Dragon Quadrilinear </small> </p>
		  </div>

		</div>


	      </li>
	    </ol>
	    
	    
      <dl class="row mt-5">
	<h1 class="display-5 col-md-8">Add on</h1>
      </dl>  
	    
      <h3> Aperture Plane </h3>
      <p>Since we are using raytracing, it's very trivial to apply aperture and focus effects on our 'new' images rendered by our virtual camera. But like the raytrcing rendering,
	it is very time consuming to render new images. The following figure on the left is the real-time aperture adjustment while on the right is the one speeded up using software.
      </p>
      <div class="row">
	<div class="col-md-6">
	  <img src="./images/hw3/gifs/focus_refocus.gif" class="img-fluid" alt="original_wireframe">
	  <p class = "text-center m-auto"><small>Real Speed </small> </p>
	</div>
	<div class="col-md-6">
	  <img src="./images/hw3/gifs/focus_refocus_10x.gif" class="img-fluid" alt="original_wireframe">
	  <p class = "text-center m-auto"><small>10X speed up </small> </p>
	</div>
	    
    </div>

    
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  </body>
</html>
